
perf problem with storing locations of each page in parent pages?
    seek doesn't need them.
    next/prev doesn't need them.
    so most of the time, they're not needed.
    but they take up space, so lots fewer things fit in a parent page
    which means the btree gets deeper
    store locations elsewhere?
    sidecar page for every parent page, just to store locations?

any way to represent/store keys as deltas?

need more tests of large overflows, especially with block allocation
issues, fragmentation, etc.

drop db needs to wait until threads end?  need a way to wait until
threads end?

it might be nice for merges to pass a hint down to create_segment()
which estimates how many pages will be needed so that it can
pass that hint on when it asks for a block so it can avoid using
little tiny blocks.

make sure stuff gets removed from merging when an error occurs while
writing a merge segment.

can we get rid of mergeStuff.merging?

tune block sizes for perf?

figure out proper limits for how much should be at each level

fix all the lsm tests

update rust nightly?

separate lsm code into multiple files (modules) so we can get 
better privacy on struct fields

separate merge() into find_merge() and write_merge_segment()?

rethink how merged segments are reclaimed into free space.
locking issues.  send a message to a separate thread, telling
it that the segment has become a zombie, and to reclaim it
whenever possible?

should min_segs on merges really be 2?  that means every
merge that promotes will automatically cause a merge in the
next level.

ability to merge entire file  (into what level? 2?)

ability to compact a file (write entire thing into one clean new file
with no free blocks)

diag_lsm:  dump level_sizes?  or is list_segments enough, since we
usually have only one segment per level?

implement a pending transaction manager.  allows crud operations.
accumulates them in a BTreeMap.  automatically flushes them out to
a pending segment when it gets too large.  automatically merges its
pending segments when there are more than one.  queries, automatically
putting its pending segment into the cursor.  notice when values
are actually stream and write them to disk so we don't have too
many files open.

need a function to get a cursor with a pending segment in front of it

ability to reserve a piece of each page for things like crypto

how much perf trouble is being caused by all these mutexes and Arcs?

fix fts in lsm storage

in lsm, when the root lands on a boundary, don't fetch a block only
to end up giving it back.

consider better (at higher level?) cache of elmo indexes

we're using usize all over the place for cases size and index into a page.
this is sad, because a page will never exceed 32 bits, and probably will
not exceed 16 bits.

make pgnum a u64?
same for pgnum but then storage is wasteful,
want to store as a varint, but then space in the page calculations are
more complicated.

core feature:  graveyard

reduce malloc

looks like kvp stuff should probably stay boxed.  both the key and value
always need to be held longer than one iteration, usually until the
end of the leaf, and one key for each leaf gets held longer than that.
which means if the caller provided this as a reference instead of a box,
we would have to make a copy no matter what, and if the caller had to
construct a box anyway, we would have done it twice.

a little worried about the KeyCompare always being built in to Seek.
what about cases that don't use it?

tempted to limit key size.  no overflows for keys.  key is limited by
the page size (minus overhead).  if you need a bigger key, then use a
bigger page size.

currently using trait objects Seek+Write for writing to the db.  the overhead
of dynamic dispatch is negligible compared to the IO, right?  lack of
inline optimization?  verify this.

maybe page manager should own the file (or Seek+Write) as well, and should
have a method called WritePage which the bt code would call?  this might
make it easier later to do things like witholding 16 bytes at the end for
crypto.

what if db goes out of scope with pending segments?

let's not panic (int underflow) when we try to write a segment
but the source iterator provides no pairs

https://github.com/zslayton/lifeguard

cleanup bcmp and friends

vbuf reuse write leaves

review lock ordering

keyInLeafs should share code.

same for Value and ValueRef code

wonder:  cargo bench and callgrind quick() are not the same test.
bunch is cross-crate  quick() is not.  quick() is the one that
shows the weird arena_avail calls.  bunch is the one that shows
the perf degradation.

compare_two makes far more difference than Key() and Value()

removing Overflowed from KeyRef doesn't help.  also removing
Result wrapper around KeyRef (with Overflowed removed) doesn't
help.

lto in Cargo.toml doesn't help (and no longer crashes in current
nightly)

FWIW, switching the new multicursor sort algo to compare_two makes
a big difference in perf.  compare_two is still way faster than
KeyRef, and the main difference seems to be in extra work being
done by jemalloc, for whatever reason.  also, I tried changing KeyRef
to be smaller by changing Prefixed to store a ref to the page buffer
and a cur and a len, the latter both as u16, instead of the two
bytes slices before.  this should have changed the size of that
enum case from 32 bytes to 12.  and this made very little difference
in perf, so I never committed it.

interesting to compare differences between the original compare_two
and one that simply gets two KeyRefs and calls KeyRef::cmp.

shouldn't it warn if a function returns Result but it cannot return Err?

want to write lint to disallow tabs

clean up organization of this code into modules

perhaps, when reading stuff from a buffer, instead of using a cur variable,
we should use the Read impl of slice

the benefit of getting a reference to the key or value bytes directly
in the page will be diminished when the bytes are compressed or encrypted.

chg names back to Key and Value?

read bson value without alloc?  but then we need to give references into
the buffer, which might be big.  that's basically read bson value with
only one (big) alloc for the object itself.

TODO this cursor needs to expose the changeCounter and segment list
on which it is based. for optimistic writes.  caller can grab a cursor,
do their writes, then grab the writelock, and grab another cursor, then
compare the two cursors to see if anything important changed.  if not,
commit their writes.  if so, nevermind the written segments and start over.



