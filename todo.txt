
do we need a Page class like PageCursor which can represent either
LeafPage or ParentPage, with a common set of methods?

LeafPage and ParentPage now need methods to return the key range and
the complete block list.

get rid of segments_info?  blocks for a page can be gotten from the page.

get rid of SegmentNum?

header can get really small.  so small that we no longer want to 
waste 4096 bytes or an entire page on it.  it needs to live on
a portion of the first page, with the rest available as a short
page for use by segments.  which means the PageWriter stuff needs
to know that not all pages are the same size (in terms of their
usable space).  and create_segment() needs to know how big its
next page is going to be.

can track open cursors (as read locks to prevent replaced segments
for being freed yet) by page now.  but we can't drop those read
locks on read_page(), letting each one go as we walk through
a full segment with Next(), because somebody might call Prev()
or First().  and we probably don't want to add thousands of
little locks.  rather, we need the notion of "I have a lock on
this page and all the pages it uses."  Kind of like what we
have now, but works on the page number.

merge:

pick the lowest single page in dest level that entirely
contains the stuff being merged from younger levels.  
that page is X.  parent is Y.

what if X is a leaf?  it can be, right?

if X doesn't exist?

if X is the root, and so Y is none?

put all those things in a multicursor, and write a segment
for that cursor.  that is Z.

Z can be a leaf or a parent, regardless of what X was.
actually, no.  Y is a parent page that can only accept
Leaf or Parents, not both.  (why is that?)  so X must be
a parent page, not a leaf, and if Z ends up being a leaf,
it needs a parent wrapper around it so it can go into Y.
alternatively, parent page can learn how to have mixed
children, some leaf, some parent.

if X did not exist, maybe Z is a new sibling of the root, 
so we need a new parent/root.

otherwise, rewrite Y, replacing X with Z.  and then rewrite
Y's parent, and so on, up to the root.

X and all its pages, except ones that got reused in Z,
become free pages.

what if "new Y" is not one page but two?  no problem,
just insert both of them into Y's parent where Y used to
be.  actually, this can't happen.  if Y becomes two pages,
it will get a new root and be one again.

any given parent set might be less efficient since, for
example, "new Y" might have pages that are kind of empty.

any page cursor now knows its entire block list without
reading any deeper.

ParentPageCursor will need an API to iterate over page information.

it is true that any given page (leaf or parent) can only have one
parent at a time?  but the page (on disk) does not know its parent,
because the parent can change, and we don't want to rewrite it when
that happens.

--------

do binary search of keys in the parent page like in the leaf?

not sure a parent page with only one item in it makes much sense

code for calc/build/write leaf and parent has gotten awfully similar

go back to only one key?  just keep the last key of each item?

multiple prefixes?

get rid of the blocklists?

perf problem with storing locations of each page in parent pages?
    seek doesn't need them.
    next/prev doesn't need them.
    so most of the time, they're not needed.
    but they take up space, so lots fewer things fit in a parent page
    which means the btree gets deeper
    store locations elsewhere?
    sidecar page for every parent page, just to store locations?

any way to represent/store keys as deltas?  vcdiff?

get rid of pagenum in leaf/parent cursor?  diag only?

need more tests of large overflows, especially with block allocation
issues, fragmentation, etc.

drop db needs to wait until threads end?  need a way to wait until
threads end?

it might be nice for merges to pass a hint down to create_segment()
which estimates how many pages will be needed so that it can
pass that hint on when it asks for a block so it can avoid using
little tiny blocks.

make sure stuff gets removed from merging when an error occurs while
writing a merge segment.

can we get rid of mergeStuff.merging?

tune block sizes for perf?

figure out proper limits for how much should be at each level

fix all the lsm tests

update rust nightly?

separate lsm code into multiple files (modules) so we can get 
better privacy on struct fields

separate merge() into find_merge() and write_merge_segment()?

rethink how merged segments are reclaimed into free space.
locking issues.  send a message to a separate thread, telling
it that the segment has become a zombie, and to reclaim it
whenever possible?

should min_segs on merges really be 2?  that means every
merge that promotes will automatically cause a merge in the
next level.

ability to merge entire file  (into what level? 2?)

ability to compact a file (write entire thing into one clean new file
with no free blocks)

diag_lsm:  dump level_sizes?  or is list_segments enough, since we
usually have only one segment per level?

implement a pending transaction manager.  allows crud operations.
accumulates them in a BTreeMap.  automatically flushes them out to
a pending segment when it gets too large.  automatically merges its
pending segments when there are more than one.  queries, automatically
putting its pending segment into the cursor.  notice when values
are actually stream and write them to disk so we don't have too
many files open.

need a function to get a cursor with a pending segment in front of it

ability to reserve a piece of each page for things like crypto

how much perf trouble is being caused by all these mutexes and Arcs?

fix fts in lsm storage

in lsm, when the root lands on a boundary, don't fetch a block only
to end up giving it back.

consider better (at higher level?) cache of elmo indexes

we're using usize all over the place for cases size and index into a page.
this is sad, because a page will never exceed 32 bits, and probably will
not exceed 16 bits.

make pgnum a u64?
same for pgnum but then storage is wasteful,
want to store as a varint, but then space in the page calculations are
more complicated.

core feature:  graveyard

reduce malloc

looks like kvp stuff should probably stay boxed.  both the key and value
always need to be held longer than one iteration, usually until the
end of the leaf, and one key for each leaf gets held longer than that.
which means if the caller provided this as a reference instead of a box,
we would have to make a copy no matter what, and if the caller had to
construct a box anyway, we would have done it twice.

a little worried about the KeyCompare always being built in to Seek.
what about cases that don't use it?

tempted to limit key size.  no overflows for keys.  key is limited by
the page size (minus overhead).  if you need a bigger key, then use a
bigger page size.

currently using trait objects Seek+Write for writing to the db.  the overhead
of dynamic dispatch is negligible compared to the IO, right?  lack of
inline optimization?  verify this.

maybe page manager should own the file (or Seek+Write) as well, and should
have a method called WritePage which the bt code would call?  this might
make it easier later to do things like witholding 16 bytes at the end for
crypto.

what if db goes out of scope with pending segments?

let's not panic (int underflow) when we try to write a segment
but the source iterator provides no pairs

https://github.com/zslayton/lifeguard

cleanup bcmp and friends

vbuf reuse write leaves

review lock ordering

keyInLeafs should share code.

same for Value and ValueRef code

wonder:  cargo bench and callgrind quick() are not the same test.
bunch is cross-crate  quick() is not.  quick() is the one that
shows the weird arena_avail calls.  bunch is the one that shows
the perf degradation.

compare_two makes far more difference than Key() and Value()

removing Overflowed from KeyRef doesn't help.  also removing
Result wrapper around KeyRef (with Overflowed removed) doesn't
help.

lto in Cargo.toml doesn't help (and no longer crashes in current
nightly)

FWIW, switching the new multicursor sort algo to compare_two makes
a big difference in perf.  compare_two is still way faster than
KeyRef, and the main difference seems to be in extra work being
done by jemalloc, for whatever reason.  also, I tried changing KeyRef
to be smaller by changing Prefixed to store a ref to the page buffer
and a cur and a len, the latter both as u16, instead of the two
bytes slices before.  this should have changed the size of that
enum case from 32 bytes to 12.  and this made very little difference
in perf, so I never committed it.

interesting to compare differences between the original compare_two
and one that simply gets two KeyRefs and calls KeyRef::cmp.

shouldn't it warn if a function returns Result but it cannot return Err?

want to write lint to disallow tabs

clean up organization of this code into modules

perhaps, when reading stuff from a buffer, instead of using a cur variable,
we should use the Read impl of slice

the benefit of getting a reference to the key or value bytes directly
in the page will be diminished when the bytes are compressed or encrypted.

chg names back to Key and Value?

read bson value without alloc?  but then we need to give references into
the buffer, which might be big.  that's basically read bson value with
only one (big) alloc for the object itself.

TODO this cursor needs to expose the changeCounter and segment list
on which it is based. for optimistic writes.  caller can grab a cursor,
do their writes, then grab the writelock, and grab another cursor, then
compare the two cursors to see if anything important changed.  if not,
commit their writes.  if so, nevermind the written segments and start over.



