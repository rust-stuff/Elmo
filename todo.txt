
new theory of the crime:

page P is a member of segment A

open an rlock on segment A

merge happens and segment A becomes inactive.  page P
gets recycled into segment B.

it is fine for segment A to become inactive while there
is a cursor on it.  it remains valid for reading as long
as its pages don't get freed and reused, which is what
the rlock is for.

another merge happens and segment B becomes inactive.
this time page P does not get recycled.  the merge code
puts P on the inactive list, subject to any rlocks on
segment B.  but rlocks on segment A need to matter for
page P as well.

this would not be a problem if rlocks had a blocklist,
kind of like before.

maybe segments need a way to specify that they share
paged from other segments.  for rlock purposes, you can't 
release a segment until its rlocks are gone and also any rlocks
for segments it stole its pages from.

Space could have, in addition to a list of rlocks, a list of
segment dependencies.  when you get an add_inactive for segment
X, if X is dependent on Y, zombie it.

when the last rlock for a segment gets released (say Y),
look for segments dependent on it.  

merge code knows that the dependency always comes from the
old version of the dest segment.

but overflows can be reused from any segment during merge.

--------

disable the heavy asserts and debugging code to make the
bug easier to repro.

go back a couple of commits and see where the bug got
introduced.

or maybe it has been there a long time but only now is
visible because the merge is faster.

one rwlock per segment in the header, instead of one for
the whole header?

adding more debug printfs tends to make the problem less
likely to show up.  :-(

getting/printing the blocklists at the end of prepare_merge,
seems to slow things down enough to avoid the bug.  this
change also included prints in the rlock code.

do we need an rlock on the stuff being promoted?

now seeing occasional failure of cursor1.js
leaf has invalid page type
add panic

definitely some sort of file corruption.
hard to repro.
freed a segment that was actually still in use?
pb doesn't actually have the right page in it?

inactive list still match?

make sure recycle of middle nodes is working as I think it is

parent node writer gets in trouble, overflows its stack,
when it has to deal with lots of pages that only hold one
key.

how could the following happen?  was it an overflow that newcode
thought was eaten/skipped but actually got used?

Young: inactive mismatch: segment 4641
old count: 0
new count: 1
old: BlockList { blocks: [] }
new: BlockList { blocks: [PageBlock { firstPage: 4641, lastPage: 4641 }] }
only_in_old: BlockList { blocks: [] }
only_in_new: BlockList { blocks: [PageBlock { firstPage: 4641, lastPage: 4641 }] }

occasional mis match of inactive lists.  hard to repro.
off by 1 page in one case.

once saw in inside error.

second run of test suite into same db is really slow.
probably fragmentation and blocklist stuff.

do we have no cases involving an overflowed key?

previously saw some cases where the new inactive code
had pages NOT found by the old inactive code.  that's
gotta be bad.

use the new inactive result as the final, not the old one.
but they gotta match.

on a merge from Other(n) to Other(n+1), we have been rewriting
the parent nodes but not adding the old set of parent nodes
to the inactive list.

perf problem when we run the 5M URL thing with 10x levels.
most CPU time being used by BlockList::sort_and_consolidate
within write_parent_page() called by merge, and some from
calls from prepare_merge() itself.  

this looks like a problem:

dest,Other(2),leaves_rewritten,0,leaves_recycled,45409,keys_promoted,348,keys_rewritten,0,count_parent_nodes,639

writing 639 new parent nodes just to promote a little bit of stuff into level 2.
no leaves getting rewritten.

blocklists are a problem.  the bigger the segment, the bigger
the blocklist gets.  may need to stop always dealing with the
blocklist for a whole segment at a time?  instead of subtract
blocklist to figure out what was recycled during a merge,
keep track of recycled nodes?

can we somehow get the blocklist to be smaller?  reduce fragmentation?
or is that simply a tradeoff with recycling?  in other words, if the
only way to reduce fragmentation is to rewrite more stuff during
merge, that's not good.

maybe the level factor should be less than 10x so that the segments
stay smaller and so the blocklists stay smaller.  but then reads get
slower because more segments.

can read locks go back to "segment" granularity?  open_cursor
and behind cursors are fine.  PendingMerge.now_inactive will have
to be a little different.  it is a list of stuff that got rewritten,
but those are partial segments, and we need each bit of it to be
keyed off the segment num they were owned by, so that a read lock
can prevent them from being freed.

consider fsa for storing keys instead of prefix compression.
dependency on BurntSushi fst crate?  or just extract code
parts we need and adapt?

think about memmap for cursors

unless it needs to pass along an Arc clone, methods should
be &self not &Arc<InnerPart>

might want to send a Work message as soon as the db is
loaded.  unless it was opened in a read only mode.

should we keep track of "log" segments?  written but not
put into the header yet?  for recovery?

still need solution to monster block list.

still need solution to leaf fullness problem.

file size explodes when multiple writers?  something
about pagewriter?  block size too big?  is pw.end getting
called?

vec capacities

at some point we should review which asserts we really want in release
builds and perf check without the rest of them.

can we have the segment page objects reference the buf in the
header instead of cloning it?  Arc<buf> ?  but then how do we deal
with ParentPage that wants to walk through all of its children without
realloc on each one?  separate implementations?  one form of the
page object borrows its buf (from the one in the SegmentHeaderInfo).
the other one owns its buf (so it can read a different page into it).

get rid of pagepool?

lots of bufadvance and varint read calls should probably return Result
so they can properly error on invalid pages read from the file.
or, when parsing a page, check before calling them.

need to figure out a place to call truncate_file_if_possible()
every so often.

avoiding rewrites of leaves has a downside:  once something is
high up in the file, it probably won't move, the file doesn't
shrink much, and ends up with a lot of empty blocks in it.

allow block allocation policy that always selects the earliest block?

experiment with write_merge and decisions to reuse leaves.
allow specifying the minimum number of consecutive leaves to 
be reused.

maybe everywhere we read a page (cursors), instead of a file,
we should provide an object which abstracts the reading of page.
allows a cache.  or a non-file.

revisit when to check for tombstones during write_merge

lsm_diag should just have show_page

consider fiddling with the depth at which parent nodes
store child block lists.  more than just one?  do we
need them at all?  would need parent nodes fit calculations 
to account for the size of the encoded block lists.  
do we need this anyway?

perf problem with storing locations of each page in parent pages?
    seek doesn't need them.
    next/prev doesn't need them.
    so most of the time, they're not needed.
    but they take up space, so lots fewer things fit in a parent page
    which means the btree gets deeper
    store locations elsewhere?
    sidecar page for every parent page, just to store locations?

still need to be concerned about really big segments
and merge working entirely at the granularity of a leaf.
a small merge into a segment with a million leaves will
end up rewriting a lot of parent nodes.  but if we process
the merge at a higher depth, we will end up rewriting a lot
more leaves than needed.

look for ways to make elmo index entry encoding more compact

at some point, review all decisions about what actually gets stored
in the parent page and make sure it's worth it.  two keys?  full
block list?  should we store the number of descendant leaves?

how to choose a page from the merge source when merging Other?
is a parent too big?  a leaf too small?  
rotate through the key space?

having last_key be an Option<> is so tedious

rm dead page class

rm dead MultiPageCursor?  or will we need this later for cases
where we are merging from Other level N to N+1 and want to select 
more than a leaf and less then a parent?

header can get really small.  so small that we no longer want to 
waste 4096 bytes or an entire page on it.  it needs to live on
a portion of the first page, with the rest available as a short
page for use by segments.  which means the PageWriter stuff needs
to know that not all pages are the same size (in terms of their
usable space).  and callers of PageWriter need to know how big
the next page is going to be.

do binary search of keys in the parent page like in the leaf?

not sure a parent page with only one item in it makes much sense

code for calc/build/write leaf and parent has gotten awfully similar

go back to only one key in parent page?  just keep the last key of 
each item?

multiple prefixes, so we can have better compression of keys in
parent page?

any way to represent/store keys as deltas?  vcdiff?

need more tests of large overflows, especially with block allocation
issues, fragmentation, etc.

drop db needs to wait until threads end?  need a way to wait until
threads end?

tune block sizes for perf?

figure out proper limits for how much should be at each level.
currently using geometric like leveldb, sometimes with the
factor as low as 2.

fix all the lsm tests

update rust nightly?

separate lsm code into multiple files (modules) so we can get 
better privacy on struct fields

ability to merge entire file

ability to compact a file (write entire thing into one clean new file
with no free blocks)

diag_lsm:  dump level_sizes?  or is list_segments enough, since we
usually have only one segment per level?

implement a pending transaction manager.  allows crud operations.
accumulates them in a BTreeMap.  automatically flushes them out to
a pending segment when it gets too large.  automatically merges its
pending segments when there are more than one.  queries, automatically
putting its pending segment into the cursor.  notice when values
are actually stream and write them to disk so we don't have too
many files open.

need a function to get a cursor with a pending segment in front of it

ability to reserve a piece of each page for things like crypto

how much perf trouble is being caused by all these mutexes and Arcs?

fix fts in lsm storage

consider better (at higher level?) cache of elmo indexes

we're using usize all over the place for cases size and index into a page.
this is sad, because a page will never exceed 32 bits, and probably will
not exceed 16 bits.

make pgnum a u64?
same for pgnum but then storage is wasteful,
want to store as a varint, but then space in the page calculations are
more complicated.

reduce malloc

are we using Arc too much now?

https://github.com/zslayton/lifeguard

cleanup bcmp and friends

vbuf reuse write leaves

keyInLeafs should share code.

same for Value and ValueRef code

want to write lint to disallow tabs?

perhaps, when reading stuff from a buffer, instead of using a cur variable,
we should use the Read impl of slice

the benefit of getting a reference to the key or value bytes directly
in the page will be diminished when the bytes are compressed or encrypted.

chg names back to Key and Value?

read bson value without alloc?  but then we need to give references into
the buffer, which might be big.  that's basically read bson value with
only one (big) alloc for the object itself.

TODO the cursor needs to expose the changeCounter and segment list
on which it is based. for optimistic writes.  caller can grab a cursor,
do their writes, then grab the writelock, and grab another cursor, then
compare the two cursors to see if anything important changed.  if not,
commit their writes.  if so, nevermind the written segments and start over.

