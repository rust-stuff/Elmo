
should the level multiplier be stored in the file header?

hmph.  during one of the test runs, commit_merge()
panicked because the dest segment (Other(N)) was
obliterated by the promoted stuff (had to be tombstones).

options:

(1)  make sure a segment cannot be depleted.  seems
hard.

(2)  change header.levels to Option<SegmentHeaderInfo>.
massive change.

(3)  set the root_page to 0, effectively treating 0
as None.  still a big change, but without the compiler
helping to find all the places that need to change.

(4)  remove the segment and move the other levels down.
this will probably cause the newly moved level(s) to be
desperate, and a wave of merges back upward will ensue.

(5)  write an empty leaf.  sad.  might this be the
least bad option?  but this would require a cursor
that can have First() end up not Valid().  or we
would need to special case the construction of the
cursors.

so yeah, it would seem that the blocklist stored for
the leaf children of a depth 1 parent is only used at
startup to identify all the pages in use.  is that
right?  if so, how feasible would it be to remove these
and calculate them?  any way to do this in a background
thread?

reuse key overflows.

parent node writer gets in trouble, overflows its stack,
when it has to deal with lots of pages that only hold one
key.  problem is that if only one key fits in the leaf,
then only one key fits in its parent, so the generation
of parent nodes is the same size, so depth increases
forever.  maybe we need to overflow keys unless two of
them can fit in a parent page.  or maybe a parent page
needs the ability to overflow a key even if the leaf
did not, just to ensure that at least two keys fit in
every parent.

too many open/close of the file for PageWriter as well.

need to make merges more parallelizable.  two merges in
the same level but different key range.

bufadvance and PageBuilder should use trait implementations 
for Read and Write on slices/vecs.  varint::read too.

vec capacities

can we have the segment page objects reference the buf in the
header instead of cloning it?  Arc<buf> ?  but then how do we deal
with ParentPage that wants to walk through all of its children without
realloc on each one?  separate implementations?  one form of the
page object borrows its buf (from the one in the SegmentHeaderInfo).
the other one owns its buf (so it can read a different page into it).

lots of bufadvance and varint read calls should probably return Result
so they can properly error on invalid pages read from the file.
or, when parsing a page, check before calling them.

do we have no test cases involving an overflowed key?

the multiple consecutive run of the test suite case
is basically an exercise in managing tombstones.  this
is probably not a typical workload.

it should be okay to deplete Other(N) if it is the last
level.

when using Instruments with Separate by Thread, one of the
merge threads is using a LOT more time than the others.
Probably Young->Other(0), but not certain.  this would
make sense, as merges get less common at higher levels,
right?

still might need to choose to rewrite a node that could be
recycled, based on its fullness?

it is possible for the depth of a segment to decrease
when it is the target of a merge, right?

dependencies can chain.  maybe long chains.

looping over all the children in a parent node to get
max tombstones, but when there are no tombstones, this
is sad.

5M url test spends all its time desperate, which is
unsurprising.

distinction between needs_merge() and prepare_merge()
is getting lamer.

when choosing what to promote, if there are no tombstones,
should we do random?  or should there be another thing
to prioritize?  like the one with the most items?  or
the least?

should Other levels *ever* be desperate?  why not just
let them fill up and merge later?  they don't add to
the cursor count.  TODO but if we turn this off,
on the 5M URL test, Other(0) gets huge.  maybe lock
starvation?

retry the 5M url test with lower level mulitplier

needs_merge() probably needs some way to account for whether
reads are happening or not.  if it is purely writes, we have
less incentive to do merges.  if reads are happening, we should
get desperate faster.

elmo layer still calls list_indexes way too often.  cache
this.

should the default page size be 4K or 8K ?

maybe the level factor should be less than 10x so that the segments
stay smaller and so the blocklists stay smaller.  but then reads get
slower because more segments.  nonetheless, it is interesting to
note that the 5M URL test is MUCH faster when the factor is 2
instead of 10.  why?  It's just another way of deferring writes?

unless it needs to pass along an Arc clone, methods should
be &self not &Arc<InnerPart>

where should I be using AsRef?  From/Into ?

get rid of pagepool?

so a new leaf can't squeeze between two parents anymore.
what does this mean for the merge problem where we had
parent nodes underfull and depth increasing to the right?

need to figure out a place to call truncate_file_if_possible()
every so often.

removing the rewrite_level (which should be rewrite_depth)
code now shows that the depth increasing to the right
problem is mostly better, but perhaps not entirely.
the 5M url test doesn't show it unless the consecutive
nodes for recycle setting for depth 1 is at 1 instead of 2.

status quo (one prefix for the whole page)
prefix chain
fsa

KeyRef::Prefixed
search in page without uncompressing all keys
nth key
work to get one key
cost to build

status quo is the only one that supports KeyRef::Prefixed.
most forms of compression in general will require that
the key be constructed/allocated in order to return it
for a cursor.

prefix chain is probably cheaper to build than the others.
status quo sometimes needs to go back and recalc when the
prefix len changes.  fsa will be the most expensive to 
build, probably by far.

fsa will very nicely support search in the page without
decompressing all the keys.  status quo mostly does too.  
prefix chain will not.

prefix chain won't support nth key unless we decompress
them all.  fsa won't support nth key at all.  would need
to change all uses to something more like an iterator
model.

prefix chain is ugly for overflowed keys.  is the prefix
always referring to the previous key?  even if it was
overflowed?  if so, the overflowed key always has to be
read in order to construct any keys after it.

actually, fsa has trouble with overflowed keys too.
how does an overflowed key fit in there?

could maybe just alternate?  even number keys are stored
without prefix compression.  odds ones are prefixed against
the one just before it.

the following isn't true anymore:
tree of depth 4.  if you insert a key that fits between
everything, it will end up with its own very thin parent
lineage.  one leaf, a parent with only one child,
another parent with only one child, and so on.  eventually,
it will either fit under a parent where it fits, or it
will be a sibling, causing the depth to grow.  we look
at this from the perspective of always recycling a node
when we can, but we should probably be asking a different
question.  just because a new key will fit in between
two nodes, should it?  does the answer depend on the
level?  if a level has N nodes and it can stay at N
nodes with the new key added, shouldn't we rewrite
to avoid depth increasing?

that's what the newer rewrite_level code kinda does.
it ensures that at some point not too far up, we decide
to rewrite the level, to make sure we don't have this
problem.

write merge can be a little smarter and can start at the
top for recycling decisions.  basically, when rewriting
a parent, find all the child nodes that need to be rewritten
and rewrite them, but only to the depth they were at
before.  don't allow them to increase in depth.  this
means that each one might result in more than one after
the merge.  after that, combine them with the rest of
the child nodes (the ones recycled, not being rewritten),
and write a new parent, again stopping at the depth
we started with so we can return to the caller.

but this is kinda what ParentNodeWriter is supposed to do.
why doesn't it work that way?  maybe:  if we are currently
recycling nodes at N and then we have to rewrite one, we
need to continue at N, not jump to N + 1.

maybe we just need ParentNodeWriter to stop flushing the
current just because it got a recycle of something at a
higher depth.  just pass it along, but sort all the results
by key range on done().

the other thing ParentNodeWriter got us was not having to
keep all the leaves in memory.  it's more like a stream,
processing things as we go.

if we are rewriting a node, that means we could not or
chose not to recycle the parent of that node.

every time we recycle a node, we flush the current page
in progress, at every level, down to that node's depth.

can we somehow get the blocklist to be smaller?  reduce fragmentation?
or is that simply a tradeoff with recycling?  in other words, if the
only way to reduce fragmentation is to rewrite more stuff during
merge, that's sad.

consider fsa for storing keys instead of prefix compression.
dependency on BurntSushi fst crate?  or just extract code
parts we need and adapt?

think about memmap for cursors

might want to send a Work message as soon as the db is
loaded.  unless it was opened in a read only mode.

should we keep track of "log" segments?  written but not
put into the header yet?  for recovery?

still need solution to monster block list.

file size explodes when multiple writers?  something
about pagewriter?  block size too big?  is pw.end getting
called?

at some point we should review which asserts we really want in release
builds and perf check without the rest of them.

avoiding rewrites of leaves has a downside:  once something is
high up in the file, it probably won't move, the file doesn't
shrink much, and ends up with a lot of empty blocks in it.

allow block allocation policy that always selects the earliest block?

maybe everywhere we read a page (cursors), instead of a file,
we should provide an object which abstracts the reading of page.
allows a cache.  or a non-file.

lsm_diag should just have show_page

consider fiddling with the depth at which parent nodes
store child block lists.  more than just one?  do we
need them at all?  would need parent nodes fit calculations 
to account for the size of the encoded block lists.  
do we need this anyway?

look for ways to make elmo index entry encoding more compact

at some point, review all decisions about what actually gets stored
in the parent page and make sure it's worth it.  full
block list?  should we store the number of descendant leaves?

header can get really small.  so small that we no longer want to 
waste 4096 bytes or an entire page on it?  it would need to live on
a portion of the first page, with the rest available as a short
page for use by segments.  which means the PageWriter stuff needs
to know that not all pages are the same size (in terms of their
usable space).  and callers of PageWriter need to know how big
the next page is going to be.

do binary search of keys in the parent page like in the leaf?

code for calc/build/write leaf and parent has gotten awfully similar

multiple prefixes, so we can have better compression of keys in
parent page?

any way to represent/store keys as deltas?  vcdiff?

need more tests of large overflows, especially with block allocation
issues, fragmentation, etc.

drop db needs to wait until threads end?  need a way to wait until
threads end?

tune block sizes for perf?

figure out proper limits for how much should be at each level.
currently using geometric like leveldb, sometimes with the
factor as low as 2.  lower multipler means faster writes and
slower reads.

update rust nightly?

separate lsm code into multiple files (modules) so we can get 
better privacy on struct fields

ability to merge entire file

ability to compact a file (write entire thing into one clean new file
with no free blocks)

diag_lsm:  dump level_sizes?  or is list_segments enough, since we
usually have only one segment per level?

implement a pending transaction manager.  allows crud operations.
accumulates them in a BTreeMap.  automatically flushes them out to
a pending segment when it gets too large.  automatically merges its
pending segments when there are more than one.  queries, automatically
putting its pending segment into the cursor.  notice when values
are actually stream and write them to disk so we don't have too
many files open.

need a function to get a cursor with a pending segment in front of it

ability to reserve a piece of each page for things like crypto

how much perf trouble is being caused by all these mutexes and Arcs?

fix fts in lsm storage

consider better (at higher level?) cache of elmo indexes

we're using usize all over the place for cases size and index into a page.
this is sad, because a page will never exceed 32 bits, and probably will
not exceed 16 bits.

make pgnum a u64?
same for pgnum but then storage is wasteful,
want to store as a varint, but then space in the page calculations are
more complicated.

reduce malloc

are we using Arc too much now?  are there places we could sweep back
through and replace Arc with &T ?

https://github.com/zslayton/lifeguard

cleanup bcmp and friends

vbuf reuse write leaves

keyInLeafs should share code.

same for Value and ValueRef code

perhaps, when reading stuff from a buffer, instead of using a cur variable,
we should use the Read impl of slice

the benefit of getting a reference to the key or value bytes directly
in the page will be diminished when the bytes are compressed or encrypted.

chg names back to Key and Value?

read bson value without alloc?  but then we need to give references into
the buffer, which might be big.  that's basically read bson value with
only one (big) alloc for the object itself.

TODO the cursor needs to expose the changeCounter and segment list
on which it is based. for optimistic writes.  caller can grab a cursor,
do their writes, then grab the writelock, and grab another cursor, then
compare the two cursors to see if anything important changed.  if not,
commit their writes.  if so, nevermind the written segments and start over.

