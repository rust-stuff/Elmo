
need to make merges more parallelizable.  two merges in
the same level but different key range.

need to not open/close the file for writing so often.

write merge can be a little smarter and can start at the
top for recycling decisions.  basically, when rewriting
a parent, find all the child nodes that need to be rewritten
and rewrite them, but only to the depth they were at
before.  don't allow them to increase in depth.  this
means that each one might result in more than one after
the merge.  after that, combine them with the rest of
the child nodes (the ones recycled, not being rewritten),
and write a new parent, again stopping at the depth
we started with so we can return to the caller.

but this is kinda what ParentNodeWriter is supposed to do.
why doesn't it work that way?  maybe:  if we are currently
recycling nodes at N and then we have to rewrite one, we
need to continue at N, not jump to N + 1.

maybe we just need ParentNodeWriter to stop flushing the
current just because it got a recycle of something at a
higher depth.  just pass it along, but sort all the results
by key range on done().

commit 5M urls in one transaction.  we will need ways of
dealing with large segments in fresh and young.  can't
promote whole segment.  writing segment would need to be
done in batches, but can't keep page in ram for lots of
batches.  in this sense, promote from young needs to be
not much different than promote from N.

and maybe promote from N to N+1 needs the ability to
leave N empty (edge case).

verify that the new code does inactive stuff right

maybe the level factor should be less than 10x so that the segments
stay smaller and so the blocklists stay smaller.  but then reads get
slower because more segments.  nonetheless, it is interesting to
note that the 5M URL test is MUCH faster when the factor is 2
instead of 10.  why?

if we are rewriting a node, that means we could not or
chose not to recycle the parent of that node.

every time we recycle a node, we flush the current page
in progress, at every level, down to that node's depth.

parent node writer gets in trouble, overflows its stack,
when it has to deal with lots of pages that only hold one
key.

second run of test suite into same db is really slow.
probably fragmentation and blocklist stuff?

do we have no test cases involving an overflowed key?

can we somehow get the blocklist to be smaller?  reduce fragmentation?
or is that simply a tradeoff with recycling?  in other words, if the
only way to reduce fragmentation is to rewrite more stuff during
merge, that's sad.

consider fsa for storing keys instead of prefix compression.
dependency on BurntSushi fst crate?  or just extract code
parts we need and adapt?

think about memmap for cursors

unless it needs to pass along an Arc clone, methods should
be &self not &Arc<InnerPart>

might want to send a Work message as soon as the db is
loaded.  unless it was opened in a read only mode.

should we keep track of "log" segments?  written but not
put into the header yet?  for recovery?

still need solution to monster block list.

file size explodes when multiple writers?  something
about pagewriter?  block size too big?  is pw.end getting
called?

vec capacities

at some point we should review which asserts we really want in release
builds and perf check without the rest of them.

can we have the segment page objects reference the buf in the
header instead of cloning it?  Arc<buf> ?  but then how do we deal
with ParentPage that wants to walk through all of its children without
realloc on each one?  separate implementations?  one form of the
page object borrows its buf (from the one in the SegmentHeaderInfo).
the other one owns its buf (so it can read a different page into it).

get rid of pagepool?

lots of bufadvance and varint read calls should probably return Result
so they can properly error on invalid pages read from the file.
or, when parsing a page, check before calling them.

need to figure out a place to call truncate_file_if_possible()
every so often.

avoiding rewrites of leaves has a downside:  once something is
high up in the file, it probably won't move, the file doesn't
shrink much, and ends up with a lot of empty blocks in it.

allow block allocation policy that always selects the earliest block?

maybe everywhere we read a page (cursors), instead of a file,
we should provide an object which abstracts the reading of page.
allows a cache.  or a non-file.

revisit when to check for tombstones during write_merge

lsm_diag should just have show_page

consider fiddling with the depth at which parent nodes
store child block lists.  more than just one?  do we
need them at all?  would need parent nodes fit calculations 
to account for the size of the encoded block lists.  
do we need this anyway?

perf problem with storing locations of each page in parent pages?
    seek doesn't need them.
    next/prev doesn't need them.
    so most of the time, they're not needed.
    but they take up space, so lots fewer things fit in a parent page
    which means the btree gets deeper
    store locations elsewhere?
    sidecar page for every parent page, just to store locations?

look for ways to make elmo index entry encoding more compact

at some point, review all decisions about what actually gets stored
in the parent page and make sure it's worth it.  two keys?  full
block list?  should we store the number of descendant leaves?

how to choose a page from the merge source when merging Other?
is a parent too big?  a leaf too small?  
rotate through the key space?

having last_key be an Option<> is so tedious

rm dead MultiPageCursor?  or will we need this later for cases
where we are merging from Other level N to N+1 and want to select 
more than a leaf and less then a parent?

header can get really small.  so small that we no longer want to 
waste 4096 bytes or an entire page on it.  it needs to live on
a portion of the first page, with the rest available as a short
page for use by segments.  which means the PageWriter stuff needs
to know that not all pages are the same size (in terms of their
usable space).  and callers of PageWriter need to know how big
the next page is going to be.

do binary search of keys in the parent page like in the leaf?

not sure a parent page with only one item in it makes much sense

code for calc/build/write leaf and parent has gotten awfully similar

multiple prefixes, so we can have better compression of keys in
parent page?

any way to represent/store keys as deltas?  vcdiff?

need more tests of large overflows, especially with block allocation
issues, fragmentation, etc.

drop db needs to wait until threads end?  need a way to wait until
threads end?

tune block sizes for perf?

figure out proper limits for how much should be at each level.
currently using geometric like leveldb, sometimes with the
factor as low as 2.  lower multipler means faster writes and
slower reads.

fix/rewrite all the lsm cargo tests

update rust nightly?

separate lsm code into multiple files (modules) so we can get 
better privacy on struct fields

ability to merge entire file

ability to compact a file (write entire thing into one clean new file
with no free blocks)

diag_lsm:  dump level_sizes?  or is list_segments enough, since we
usually have only one segment per level?

implement a pending transaction manager.  allows crud operations.
accumulates them in a BTreeMap.  automatically flushes them out to
a pending segment when it gets too large.  automatically merges its
pending segments when there are more than one.  queries, automatically
putting its pending segment into the cursor.  notice when values
are actually stream and write them to disk so we don't have too
many files open.

need a function to get a cursor with a pending segment in front of it

ability to reserve a piece of each page for things like crypto

how much perf trouble is being caused by all these mutexes and Arcs?

fix fts in lsm storage

consider better (at higher level?) cache of elmo indexes

we're using usize all over the place for cases size and index into a page.
this is sad, because a page will never exceed 32 bits, and probably will
not exceed 16 bits.

make pgnum a u64?
same for pgnum but then storage is wasteful,
want to store as a varint, but then space in the page calculations are
more complicated.

reduce malloc

are we using Arc too much now?  are there places we could sweep back
through and replace Arc with &T ?

https://github.com/zslayton/lifeguard

cleanup bcmp and friends

vbuf reuse write leaves

keyInLeafs should share code.

same for Value and ValueRef code

want to write lint to disallow tabs?

perhaps, when reading stuff from a buffer, instead of using a cur variable,
we should use the Read impl of slice

the benefit of getting a reference to the key or value bytes directly
in the page will be diminished when the bytes are compressed or encrypted.

chg names back to Key and Value?

read bson value without alloc?  but then we need to give references into
the buffer, which might be big.  that's basically read bson value with
only one (big) alloc for the object itself.

TODO the cursor needs to expose the changeCounter and segment list
on which it is based. for optimistic writes.  caller can grab a cursor,
do their writes, then grab the writelock, and grab another cursor, then
compare the two cursors to see if anything important changed.  if not,
commit their writes.  if so, nevermind the written segments and start over.

