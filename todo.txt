
diag, count keys, avg keys per leaf, count parent nodes,
avg keys per parent node, etc

how can something go from 190,000 pages to so much fewer?
are we ending up with lots of leaves that have lots of empty
space in them?

merge should release the merge lock each time through
the loop so others get a chance

need a way to report how many parent nodes got written
by a merge.  and how many keys got processed from the
junior section.

file size explodes when multiple writers?  something
about pagewriter?  block size too big?  is pw.end getting
called?

fragmentation is getting awful.  really bad.

remaining siblings code is just kinda wrong

merge 0 just went from 
    46371, 22697 pages
to
    46696, 395 pages
    46695, 57 pages
which has gotta be wrong.

merge fresh doesn't seem to lose keys.
merge young is fine too.
merge 0 loses a lot.

initial count looks wrong too.  after 1000 runs of 1000 keys, we should
have a millon, and we had 977,561.  hmmm. could be updates overwrote
because we accidentally chosen the same key as before.

vec capacities

prepare_merge clones too much stuff

at some point we should review which asserts we really want in release
builds and perf check without the rest of them.

can we have the segment page objects reference the buf in the
header instead of cloning it?

get rid of pagepool?

lots of bufadvance and varint read calls should probably return Result
so they can properly error on invalid pages read from the file.
or, when parsing a page, check before calling them.

need to figure out a place to call truncate_file_if_possible()
every so often.

avoiding rewrites of leaves has a downside:  once something is
high up in the file, it probably won't move, the file doesn't
shrink much, and ends up with a lot of empty blocks in it.

allow block allocation policy that always selects the earliest block?

experiment with write_merge and decisions to reuse leaves.
allow specifying the minimum number of consecutive leaves to 
be reused.

maybe everywhere we read a page (cursors), instead of a file,
we should provide an object which abstracts the reading of page.
allows a cache.  or a non-file.

revisit when to check for tombstones during write_merge

need finer grained merge locks?

lsm_diag should just have show_page

consider fiddling with the depth at which parent nodes
store child block lists.  more than just one?  do we
need them at all?  would need parent nodes fit calculations 
to account for the size of the encoded block lists.  
do we need this anyway?

perf problem with storing locations of each page in parent pages?
    seek doesn't need them.
    next/prev doesn't need them.
    so most of the time, they're not needed.
    but they take up space, so lots fewer things fit in a parent page
    which means the btree gets deeper
    store locations elsewhere?
    sidecar page for every parent page, just to store locations?

still need to be concerned about really big segments
and merge working entirely at the granularity of a leaf.
a small merge into a segment with a million leaves will
end up rewriting a lot of parent nodes.  but if we process
the merge at a higher depth, we will end up rewriting a lot
more leaves than needed.

look for ways to make elmo index entry encoding more compact

at some point, review all decisions about what actually gets stored
in the parent page and make sure it's worth it.  two keys?  full
block list?  should we store the number of descendant leaves?

how to choose a page from the merge source when merging Other?
is a parent too big?  a leaf too small?  
rotate through the key space?

having last_key be an Option<> is so tedious

rm dead page class

rm dead MultiPageCursor?  or will we need this later for cases
where we are merging from Other level N to N+1 and want to select 
more than a leaf and less then a parent?

header can get really small.  so small that we no longer want to 
waste 4096 bytes or an entire page on it.  it needs to live on
a portion of the first page, with the rest available as a short
page for use by segments.  which means the PageWriter stuff needs
to know that not all pages are the same size (in terms of their
usable space).  and callers of PageWriter need to know how big
the next page is going to be.

do binary search of keys in the parent page like in the leaf?

not sure a parent page with only one item in it makes much sense

code for calc/build/write leaf and parent has gotten awfully similar

go back to only one key in parent page?  just keep the last key of 
each item?

multiple prefixes, so we can have better compression of keys in
parent page?

any way to represent/store keys as deltas?  vcdiff?

need more tests of large overflows, especially with block allocation
issues, fragmentation, etc.

drop db needs to wait until threads end?  need a way to wait until
threads end?

tune block sizes for perf?

figure out proper limits for how much should be at each level.
currently using geometric like leveldb, sometimes with the
factor as low as 2.

fix all the lsm tests

update rust nightly?

separate lsm code into multiple files (modules) so we can get 
better privacy on struct fields

ability to merge entire file

ability to compact a file (write entire thing into one clean new file
with no free blocks)

diag_lsm:  dump level_sizes?  or is list_segments enough, since we
usually have only one segment per level?

implement a pending transaction manager.  allows crud operations.
accumulates them in a BTreeMap.  automatically flushes them out to
a pending segment when it gets too large.  automatically merges its
pending segments when there are more than one.  queries, automatically
putting its pending segment into the cursor.  notice when values
are actually stream and write them to disk so we don't have too
many files open.

need a function to get a cursor with a pending segment in front of it

ability to reserve a piece of each page for things like crypto

how much perf trouble is being caused by all these mutexes and Arcs?

fix fts in lsm storage

consider better (at higher level?) cache of elmo indexes

we're using usize all over the place for cases size and index into a page.
this is sad, because a page will never exceed 32 bits, and probably will
not exceed 16 bits.

make pgnum a u64?
same for pgnum but then storage is wasteful,
want to store as a varint, but then space in the page calculations are
more complicated.

reduce malloc

are we using Arc too much now?

https://github.com/zslayton/lifeguard

cleanup bcmp and friends

vbuf reuse write leaves

keyInLeafs should share code.

same for Value and ValueRef code

want to write lint to disallow tabs?

perhaps, when reading stuff from a buffer, instead of using a cur variable,
we should use the Read impl of slice

the benefit of getting a reference to the key or value bytes directly
in the page will be diminished when the bytes are compressed or encrypted.

chg names back to Key and Value?

read bson value without alloc?  but then we need to give references into
the buffer, which might be big.  that's basically read bson value with
only one (big) alloc for the object itself.

TODO the cursor needs to expose the changeCounter and segment list
on which it is based. for optimistic writes.  caller can grab a cursor,
do their writes, then grab the writelock, and grab another cursor, then
compare the two cursors to see if anything important changed.  if not,
commit their writes.  if so, nevermind the written segments and start over.

